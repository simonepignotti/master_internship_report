
\chapter{Introduction}
\label{Chapter1}

\section{Context and Motivations}

Even before understanding the structure and function of DNA, humans have advanced hypothesis on how certain traits can be inherited over generations. While there is still much to be learnt about this molecule, the advent of molecular biology and bioinformatics marked the beginning of a new era, deeply changing people's perception of life. Technological advances enabled the analysis of DNA and other biological molecules, whose sequences can now be observed and for which we found a convenient representation as strings over a small fixed alphabet. Such representation enables their computational analysis, often using algorithms which were originally conceived for the analysis of text.

Starting from simple microbial organisms, researchers started to assemble the short genomic sequences produced by sequencing technologies into genes and chromosomes, till the entire human genome has been characterized in 2001. Nowadays, genomic databases like NCBI's GenBank contain billions of sequences, and in the case of GenBank its size is doubling approximately every 18 months \footnote{~\url{https://www.ncbi.nlm.nih.gov/genbank/statistics/}}. Several algorithmic challenges were introduced by the amount of sequencing data being generated, spanning from compression and storage to annotation and comparison.

The incredible amount of information stored in these sequences is far from being extracted. Algorithmic challenges and technological limitations make the comprehensive analysis of all the produced sequencing data impossible at the current state, and including every such sequence in a genomic ``search engine'' is unfeasible even with the latest methods and the advances in cloud computing. The potential of these data for understanding human diseases and environmental issues is astonishing, and there is a real need for the development of efficient algorithms and data structures for this most relevant field of Big Data.

Nevertheless, progress in bioinformatics created the possibility to study entire populations and communities of hundreds of different microbial species without the need to isolate single clonal populations or cells. In metagenomics, the genetic material extracted from environmental samples is sequenced and the resulting short fragments of DNA, called \textit{reads}, are assigned to an extensive database of reference genomes. The size of the datasets, the mutations occurring at a fast pace and the presence of organisms which have never been sequenced yet make the use of standard mapping algorithms not fit for this scenario. Therefore new methods are being developed which carry out the task of assigning short sequences to the genomes of the organisms they are most likely to be originated from, using heuristics for speeding up the computation, decreasing the space complexity and estimating the composition of the sample.

Here we focus on the estimation of relative abundances of microbial organisms in metagenomic samples, and we introduce a method to probabilistically distribute the assignments of the metagenomic classifiers, in particular those of our software ProPhyle. We use a linear regression model to account for the similarities of the reference genomes and solve the problem of reads which map equally well to multiple reference genomes. We show that the method provides accurate results and is comparable to other state-of-the-art abundance estimation tools.

In the rest of this chapter we introduce notions about the analysis of genomic sequences and we provide a background for readers who are not keen to bioinformatics. In Chapter \ref{Chapter2} we analyze relevant programs and algorithms performing both read assignment and abundance estimation. In Chapter \ref{Chapter3} we introduce the method we propose and finally in Chapter \ref{Chapter4} we show the results obtained on different datasets.

\section{Analysis of DNA sequences}

% NGS data
Even though they may sound like fairly simple organisms, the genomes of most bacterial species are composed of millions of nucleotides. Obtaining a faithful representation of them is until today a big challenge, mostly because modern sequencing technologies only provide short fragments (\textit{reads}) of this long chain, usually in the order of few hundred base pairs, or letters; while cutting-edge tools may produce longer sequences, this is at the cost of introducing a considerable amount of sequencing errors. In both cases, reads need to be assembled like puzzles to recover the original genome of the organism, a task which requires a good coverage of the genome and that is made more complex by the presence of highly repetitive portions called \textit{low-complexity regions}. In reference-based metagenomics, a database of high quality reference genomes is needed for the assignments not to be biased by i.e. contamination from other organisms or poor assembly.

% Sequence Alignment
Indeed, the reads of the metagenome need to be compared to a set of reference genomes in order to estimate the composition of the sample. The first algorithms to perform the comparison of two sequences were based on dynamic programming: the input sequences were aligned in such a way to minimize the number of mismatches, insertions or deletions to transform one sequence into the other. These algorithms soon became computationally unfeasible due to the fact that unforeseen amounts of data were being generated, that they only allowed pairwise comparison, and that their cost was quadratic in the size of the input sequences. In the current setting, the analysis of metagenomic reads with such a tool would require years; furthermore, the actual alignment is not needed to assess the composition of the sample, and we are mainly interested in which genome the read is originated from.

% Alignment Free
Soon heuristic methods were developed for sequence comparison, whose BLAST is probably the best representative and is therefore analyzed in Chapter \ref{Chapter2}. Most importantly, the alignments are performed only for those reference genomes which have an exact match for a subsequence in the read, drastically reducing the number of pairwise comparisons. In the context of metagenomics though, even tools like BLAST, which are still used in other omics fields, became unfeasible for the current size of experiments. Pushing the idea behind BLAST's so-called ``\textit{seed-and-extend}'' paradigm, alignment-free algorithms started to take its place. In alignment-free sequence comparison, sequences are viewed as sets of substrings of fixed size $k$, called $k$-mers, which can be indexed and queried promptly. Reads are then assigned to genomes sharing enough $k$-mers with them. This similarity measure, surely weaker than alignment score, allows the analysis of huge metagenomic samples in a fraction of the time: as an example, the software Kraken (also analyzed in Chapter \ref{Chapter2}) reaches assignment speeds of up to a million reads per minute on databases containing thousands of reference genomes.

This performance is achieved also thanks to the organization of sequences in an evolutionary tree reflecting sequence similarity, called \textit{phylogenetic} tree, which is commonly used in metagenomics to compress the references and provide abundance estimates at different level of the tree. A special kind of phylogenetic tree, called \textit{taxonomic} tree, is often used as it contains annotations like names for different levels of the tree, reflecting the characteristics of the subtree (i.e. species, genera, families). Since genomes in the same species share most of their genetic material, reads may now be assigned to internal nodes of the tree; furthermore, biological mechanisms like horizontal gene transfer, allowing bacteria of different species to exchange portions of their DNA, make assignments even more complex since a short read fragment may match equally well reference genomes associated to distant leaves of the tree.

%Estimation of Abundances
Estimating abundances solely from the unique mappings to the references has been shown to introduce considerable biases \cite{lu_bracken:_2017} due to the uneven representation of microbial clades in the genetic databases and their variable average similarity within those clades. Therefore methods have been developed to probabilistically redistribute assignments to multiple reference genomes or to internal nodes of the taxonomic tree to fixed ranks, like species or genus. While these methods approximate very closely the abundances of the samples at such ranks, they once again introduce biases linked to the structure of the tree, which is constant subject of discussion and dissent in the microbiology community. For this reason, and to provide better resolution to the results, modern tools focus mainly on the genome level.
