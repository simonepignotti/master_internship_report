
@phdthesis{brinda_novel_2016,
	type = {Theses},
	title = {Novel computational techniques for mapping and classification of {Next}-{Generation} {Sequencing} data},
	url = {https://hal.archives-ouvertes.fr/tel-01484198},
	urldate = {2018-08-10},
	school = {Université Paris-Est},
	author = {Brinda, Karel},
	month = nov,
	year = {2016},
	keywords = {Algorithmes, Algorithms, Classification métagenomique, Consensus calling, DNA reads, Lectures ADN, Localisation de lectures ADN, Metagenomic classification, Next Generation Sequencing, Prédiction de consensus, Read mapping, Séquençage haut débit},
	file = {HAL PDF Full Text:/home/simone/Zotero/storage/R2G9BAXA/Brinda - 2016 - Novel computational techniques for mapping and cla.pdf:application/pdf}
}

@phdthesis{salikhov_efficient_2017,
	type = {Theses},
	title = {Efficient algorithms and data structures for indexing {DNA} sequence data},
	url = {https://pastel.archives-ouvertes.fr/tel-01762479},
	urldate = {2018-08-10},
	school = {Université Paris-Est},
	author = {Salikhov, Kamil},
	month = nov,
	year = {2017},
	keywords = {Algorithmes, Algorithms, Data structures, DNA sequence data, L'indexation de séquences d'ADN, Structures de données},
	file = {HAL PDF Full Text:/home/simone/Zotero/storage/I5U6AAB4/Salikhov - 2017 - Efficient algorithms and data structures for index.pdf:application/pdf}
}

@article{brinda_spaced_2015,
	title = {Spaced seeds improve k-mer-based metagenomic classification},
	volume = {31},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/article/31/22/3584/240663},
	doi = {10.1093/bioinformatics/btv419},
	abstract = {Abstract.  Motivation: Metagenomics is a powerful approach to study genetic content of environmental samples, which has been strongly promoted by next-generatio},
	language = {en},
	number = {22},
	urldate = {2018-08-10},
	journal = {Bioinformatics},
	author = {Břinda, Karel and Sykulski, Maciej and Kucherov, Gregory},
	month = nov,
	year = {2015},
	pages = {3584--3592},
	file = {Full Text PDF:/home/simone/Zotero/storage/N9LU76UM/Břinda et al. - 2015 - Spaced seeds improve k-mer-based metagenomic class.pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/X3UVG9IG/240663.html:text/html}
}

@article{fischer_abundance_2017,
	title = {Abundance estimation and differential testing on strain level in metagenomics data},
	volume = {33},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/article/33/14/i124/3953953},
	doi = {10.1093/bioinformatics/btx237},
	abstract = {AbstractMotivation.  Current metagenomics approaches allow analyzing the composition of microbial communities at high resolution. Important changes to the compo},
	language = {en},
	number = {14},
	urldate = {2018-08-10},
	journal = {Bioinformatics},
	author = {Fischer, Martina and Strauch, Benjamin and Renard, Bernhard Y.},
	month = jul,
	year = {2017},
	pages = {i124--i132},
	file = {Full Text PDF:/home/simone/Zotero/storage/XHY5TAMV/Fischer et al. - 2017 - Abundance estimation and differential testing on s.pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/NEA45TIU/3953953.html:text/html}
}

@article{lindner_metagenomic_2013,
	title = {Metagenomic abundance estimation and diagnostic testing on species level},
	volume = {41},
	issn = {0305-1048},
	url = {https://academic.oup.com/nar/article/41/1/e10/1164154},
	doi = {10.1093/nar/gks803},
	abstract = {Abstract.  One goal of sequencing-based metagenomic community analysis is the quantitative taxonomic assessment of microbial community compositions. In particul},
	language = {en},
	number = {1},
	urldate = {2018-08-10},
	journal = {Nucleic Acids Research},
	author = {Lindner, Martin S. and Renard, Bernhard Y.},
	month = jan,
	year = {2013},
	pages = {e10--e10},
	file = {Full Text PDF:/home/simone/Zotero/storage/LS43UNIZ/Lindner and Renard - 2013 - Metagenomic abundance estimation and diagnostic te.pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/HAMCE5SI/1164154.html:text/html}
}

@article{mende_assessment_2012,
	title = {Assessment of {Metagenomic} {Assembly} {Using} {Simulated} {Next} {Generation} {Sequencing} {Data}},
	volume = {7},
	issn = {1932-6203},
	url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0031386},
	doi = {10.1371/journal.pone.0031386},
	abstract = {Due to the complexity of the protocols and a limited knowledge of the nature of microbial communities, simulating metagenomic sequences plays an important role in testing the performance of existing tools and data analysis methods with metagenomic data. We developed metagenomic read simulators with platform-specific (Sanger, pyrosequencing, Illumina) base-error models, and simulated metagenomes of differing community complexities. We first evaluated the effect of rigorous quality control on Illumina data. Although quality filtering removed a large proportion of the data, it greatly improved the accuracy and contig lengths of resulting assemblies. We then compared the quality-trimmed Illumina assemblies to those from Sanger and pyrosequencing. For the simple community (10 genomes) all sequencing technologies assembled a similar amount and accurately represented the expected functional composition. For the more complex community (100 genomes) Illumina produced the best assemblies and more correctly resembled the expected functional composition. For the most complex community (400 genomes) there was very little assembly of reads from any sequencing technology. However, due to the longer read length the Sanger reads still represented the overall functional composition reasonably well. We further examined the effect of scaffolding of contigs using paired-end Illumina reads. It dramatically increased contig lengths of the simple community and yielded minor improvements to the more complex communities. Although the increase in contig length was accompanied by increased chimericity, it resulted in more complete genes and a better characterization of the functional repertoire. The metagenomic simulators developed for this research are freely available.},
	language = {en},
	number = {2},
	urldate = {2018-08-10},
	journal = {PLOS ONE},
	author = {Mende, Daniel R. and Waller, Alison S. and Sunagawa, Shinichi and Järvelin, Aino I. and Chan, Michelle M. and Arumugam, Manimozhiyan and Raes, Jeroen and Bork, Peer},
	month = feb,
	year = {2012},
	keywords = {Dideoxy DNA sequencing, Genome analysis, Genome complexity, Genome sequencing, Invertebrate genomics, Metagenomics, Quality control, Sequence assembly tools},
	pages = {e31386},
	file = {Full Text PDF:/home/simone/Zotero/storage/4FBTGF6U/Mende et al. - 2012 - Assessment of Metagenomic Assembly Using Simulated.pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/DFSH87RN/article.html:text/html}
}

@article{lu_bracken:_2017,
	title = {Bracken: estimating species abundance in metagenomics data},
	volume = {3},
	issn = {2376-5992},
	shorttitle = {Bracken},
	url = {https://peerj.com/articles/cs-104},
	doi = {10.7717/peerj-cs.104},
	abstract = {Metagenomic experiments attempt to characterize microbial communities using high-throughput DNA sequencing. Identification of the microorganisms in a sample provides information about the genetic profile, population structure, and role of microorganisms within an environment. Until recently, most metagenomics studies focused on high-level characterization at the level of phyla, or alternatively sequenced the 16S ribosomal RNA gene that is present in bacterial species. As the cost of sequencing has fallen, though, metagenomics experiments have increasingly used unbiased shotgun sequencing to capture all the organisms in a sample. This approach requires a method for estimating abundance directly from the raw read data. Here we describe a fast, accurate new method that computes the abundance at the species level using the reads collected in a metagenomics experiment. Bracken (Bayesian Reestimation of Abundance after Classification with KrakEN) uses the taxonomic assignments made by Kraken, a very fast read-level classifier, along with information about the genomes themselves to estimate abundance at the species level, the genus level, or above. We demonstrate that Bracken can produce accurate species- and genus-level abundance estimates even when a sample contains multiple near-identical species.},
	language = {en},
	urldate = {2018-08-10},
	journal = {PeerJ Computer Science},
	author = {Lu, Jennifer and Breitwieser, Florian P. and Thielen, Peter and Salzberg, Steven L.},
	month = jan,
	year = {2017},
	pages = {e104},
	file = {Full Text PDF:/home/simone/Zotero/storage/47W3LIFK/Lu et al. - 2017 - Bracken estimating species abundance in metagenom.pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/MG35LCEZ/cs-104.html:text/html}
}

@article{bray_near-optimal_2016,
	title = {Near-optimal probabilistic {RNA}-seq quantification},
	volume = {34},
	copyright = {2016 Nature Publishing Group},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/nbt.3519},
	doi = {10.1038/nbt.3519},
	abstract = {We present kallisto, an RNA-seq quantification program that is two orders of magnitude faster than previous approaches and achieves similar accuracy. Kallisto pseudoaligns reads to a reference, producing a list of transcripts that are compatible with each read while avoiding alignment of individual bases. We use kallisto to analyze 30 million unaligned paired-end RNA-seq reads in {\textless}10 min on a standard laptop computer. This removes a major computational bottleneck in RNA-seq analysis.},
	language = {en},
	number = {5},
	urldate = {2018-08-10},
	journal = {Nature Biotechnology},
	author = {Bray, Nicolas L. and Pimentel, Harold and Melsted, Páll and Pachter, Lior},
	month = may,
	year = {2016},
	pages = {525--527},
	file = {Bray et al. - 2016 - Near-optimal probabilistic RNA-seq quantification.pdf:/home/simone/Zotero/storage/PJVZFNV9/Bray et al. - 2016 - Near-optimal probabilistic RNA-seq quantification.pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/UW8PYVET/nbt.html:text/html}
}

@article{wood_kraken:_2014,
	title = {Kraken: ultrafast metagenomic sequence classification using exact alignments},
	volume = {15},
	issn = {1474-760X},
	shorttitle = {Kraken},
	url = {https://doi.org/10.1186/gb-2014-15-3-r46},
	doi = {10.1186/gb-2014-15-3-r46},
	abstract = {Kraken is an ultrafast and highly accurate program for assigning taxonomic labels to metagenomic DNA sequences. Previous programs designed for this task have been relatively slow and computationally expensive, forcing researchers to use faster abundance estimation programs, which only classify small subsets of metagenomic data. Using exact alignment of k-mers, Kraken achieves classification accuracy comparable to the fastest BLAST program. In its fastest mode, Kraken classifies 100 base pair reads at a rate of over 4.1 million reads per minute, 909 times faster than Megablast and 11 times faster than the abundance estimation program MetaPhlAn. Kraken is available at                   http://ccb.jhu.edu/software/kraken/                                  .},
	number = {3},
	urldate = {2018-08-10},
	journal = {Genome Biology},
	author = {Wood, Derrick E. and Salzberg, Steven L.},
	month = mar,
	year = {2014},
	pages = {R46},
	file = {Full Text PDF:/home/simone/Zotero/storage/T44R6HSU/Wood and Salzberg - 2014 - Kraken ultrafast metagenomic sequence classificat.pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/EJ46C9D4/gb-2014-15-3-r46.html:text/html}
}

@article{kim_centrifuge:_2016,
	title = {Centrifuge: rapid and sensitive classification of metagenomic sequences},
	issn = {1088-9051, 1549-5469},
	shorttitle = {Centrifuge},
	url = {http://genome.cshlp.org/content/early/2016/11/16/gr.210641.116},
	doi = {10.1101/gr.210641.116},
	abstract = {Centrifuge is a novel microbial classification engine that enables rapid, accurate, and sensitive labeling of reads and quantification of species on desktop computers. The system uses an indexing scheme based on the Burrows-Wheeler transform (BWT) and the Ferragina-Manzini (FM) index, optimized specifically for the metagenomic classification problem. Centrifuge requires a relatively small index (4.2 GB for 4078 bacterial and 200 archaeal genomes) and classifies sequences at very high speed, allowing it to process the millions of reads from a typical high-throughput DNA sequencing run within a few minutes. Together, these advances enable timely and accurate analysis of large metagenomics data sets on conventional desktop computers. Because of its space-optimized indexing schemes, Centrifuge also makes it possible to index the entire NCBI nonredundant nucleotide sequence database (a total of 109 billion bases) with an index size of 69 GB, in contrast to k-mer-based indexing schemes, which require far more extensive space.},
	language = {en},
	urldate = {2018-08-10},
	journal = {Genome Research},
	author = {Kim, Daehwan and Song, Li and Breitwieser, Florian P. and Salzberg, Steven L.},
	month = oct,
	year = {2016},
	pmid = {27852649},
	file = {Full Text PDF:/home/simone/Zotero/storage/8CZWTEKS/Kim et al. - 2016 - Centrifuge rapid and sensitive classification of .pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/2STSFY4F/gr.210641.116.html:text/html}
}

@article{lindgreen_evaluation_2016,
	title = {An evaluation of the accuracy and speed of metagenome analysis tools},
	volume = {6},
	copyright = {2016 Nature Publishing Group},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/srep19233},
	doi = {10.1038/srep19233},
	abstract = {Metagenome studies are becoming increasingly widespread, yielding important insights into microbial communities covering diverse environments from terrestrial and aquatic ecosystems to human skin and gut. With the advent of high-throughput sequencing platforms, the use of large scale shotgun sequencing approaches is now commonplace. However, a thorough independent benchmark comparing state-of-the-art metagenome analysis tools is lacking. Here, we present a benchmark where the most widely used tools are tested on complex, realistic data sets. Our results clearly show that the most widely used tools are not necessarily the most accurate, that the most accurate tool is not necessarily the most time consuming, and that there is a high degree of variability between available tools. These findings are important as the conclusions of any metagenomics study are affected by errors in the predicted community composition and functional capacity. Data sets and results are freely available from http://www.ucbioinformatics.org/metabenchmark.html},
	language = {en},
	urldate = {2018-08-10},
	journal = {Scientific Reports},
	author = {Lindgreen, Stinus and Adair, Karen L. and Gardner, Paul P.},
	month = jan,
	year = {2016},
	pages = {19233},
	file = {Full Text PDF:/home/simone/Zotero/storage/RCI85CKT/Lindgreen et al. - 2016 - An evaluation of the accuracy and speed of metagen.pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/ABQCCSKG/srep19233.html:text/html}
}

@article{ferragina_indexing_2005,
	title = {Indexing {Compressed} {Text}},
	volume = {52},
	issn = {0004-5411},
	url = {http://doi.acm.org/10.1145/1082036.1082039},
	doi = {10.1145/1082036.1082039},
	abstract = {We design two compressed data structures for the full-text indexing problem that support efficient substring searches using roughly the space required for storing the text in compressed form.Our first compressed data structure retrieves the occ occurrences of a pattern P[1,p] within a text T[1,n] in O(p + occ log1+ε n) time for any chosen ε, 0{\textless}ε{\textless}1. This data structure uses at most 5nHk(T) + o(n) bits of storage, where Hk(T) is the kth order empirical entropy of T. The space usage is Θ(n) bits in the worst case and o(n) bits for compressible texts. This data structure exploits the relationship between suffix arrays and the Burrows--Wheeler Transform, and can be regarded as a compressed suffix array.Our second compressed data structure achieves O(p+occ) query time using O(nHk(T)logε n) + o(n) bits of storage for any chosen ε, 0{\textless}ε{\textless}1. Therefore, it provides optimal output-sensitive query time using o(nlog n) bits in the worst case. This second data structure builds upon the first one and exploits the interplay between two compressors: the Burrows--Wheeler Transform and the LZ78 algorithm.},
	number = {4},
	urldate = {2018-08-10},
	journal = {J. ACM},
	author = {Ferragina, Paolo and Manzini, Giovanni},
	month = jul,
	year = {2005},
	keywords = {Burrows–Wheeler transform, full-text indexing, indexing data structure, Lempel–Ziv compressor, pattern searching, suffix array, suffix tree, text compression},
	pages = {552--581},
	file = {Ferragina and Manzini - 2005 - Indexing Compressed Text.pdf:/home/simone/Zotero/storage/I2ULZKXV/Ferragina and Manzini - 2005 - Indexing Compressed Text.pdf:application/pdf}
}

@article{ondov_mash:_2016,
	title = {Mash: fast genome and metagenome distance estimation using {MinHash}},
	volume = {17},
	issn = {1474-760X},
	shorttitle = {Mash},
	url = {https://doi.org/10.1186/s13059-016-0997-x},
	doi = {10.1186/s13059-016-0997-x},
	abstract = {Mash extends the MinHash dimensionality-reduction technique to include a pairwise mutation distance and P value significance test, enabling the efficient clustering and search of massive sequence collections. Mash reduces large sequences and sequence sets to small, representative sketches, from which global mutation distances can be rapidly estimated. We demonstrate several use cases, including the clustering of all 54,118 NCBI RefSeq genomes in 33 CPU h; real-time database search using assembled or unassembled Illumina, Pacific Biosciences, and Oxford Nanopore data; and the scalable clustering of hundreds of metagenomic samples by composition. Mash is freely released under a BSD license (                  https://github.com/marbl/mash                                  ).},
	number = {1},
	urldate = {2018-08-10},
	journal = {Genome Biology},
	author = {Ondov, Brian D. and Treangen, Todd J. and Melsted, Páll and Mallonee, Adam B. and Bergman, Nicholas H. and Koren, Sergey and Phillippy, Adam M.},
	month = jun,
	year = {2016},
	pages = {132},
	file = {Full Text PDF:/home/simone/Zotero/storage/3JUSJ5I4/Ondov et al. - 2016 - Mash fast genome and metagenome distance estimati.pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/7M5TKSZV/s13059-016-0997-x.html:text/html}
}

@article{dilthey_metamaps_2018,
	title = {{MetaMaps} - {Strain}-level metagenomic assignment and compositional estimation for long reads},
	copyright = {© 2018, Posted by Cold Spring Harbor Laboratory. This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available for use under a CC0 license},
	url = {https://www.biorxiv.org/content/early/2018/07/20/372474},
	doi = {10.1101/372474},
	abstract = {Metagenomic sequence classification should be fast, accurate and information-rich. Emerging long-read sequencing technologies promise to improve the balance between these factors but most existing methods were designed for short reads. MetaMaps is a new method, specifically developed for long reads, that combines the accuracy of slower alignment-based methods with the scalability of faster k-mer-based methods. Using an approximate mapping algorithm, it is capable of mapping a long-read metagenome to a comprehensive RefSeq database with {\textgreater}12,000 genomes in {\textless}30 GB or RAM on a laptop computer. Integrating these mappings with a probabilistic scoring scheme and EM-based estimation of sample composition, MetaMaps achieves {\textgreater}95\% accuracy for species-level read assignment and r2 {\textgreater} 0.98 for the estimation of sample composition on both simulated and real data. Uniquely, MetaMaps outputs mapping locations and qualities for all classified reads, enabling functional studies (e.g. gene presence/absence) and the detection of novel species not present in the current database.},
	language = {en},
	urldate = {2018-08-10},
	journal = {bioRxiv},
	author = {Dilthey, Alexander and Jain, Chirag and Koren, Sergey and Phillippy, Adam},
	month = jul,
	year = {2018},
	pages = {372474},
	file = {Full Text PDF:/home/simone/Zotero/storage/AYZU9X37/Dilthey et al. - 2018 - MetaMaps - Strain-level metagenomic assignment and.pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/YU7ABQNS/372474.html:text/html}
}

@article{nasko_refseq_2018,
	title = {{RefSeq} database growth influences the accuracy of k-mer-based species identification},
	copyright = {© 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/early/2018/04/19/304972},
	doi = {10.1101/304972},
	abstract = {Accurate species-level taxonomic classification and profiling of complex microbial communities remains a challenge due to homologous regions shared among closely related species and a sparse representation of non-human associated microbes in the database. Although the database undoubtedly has a strong influence on the sensitivity of taxonomic classifiers and profilers, to date, no study has carefully explored this topic on historical RefSeq releases and explored its impact on accuracy. In this study, we examined the influence of the database, over time, on k-mer based sequence classification and profiling. We present three major findings: (i) database growth over time resulted in more classified reads, but fewer species-level classifications and more species-level misclassifications; (ii) Bayesian re-estimation of abundance helped to recover species-level classifications when the exact target strain was present; and (iii) Bayesian re-estimation struggled when the database lacked the target strain, resulting in a notable decrease in accuracy. In summary, our findings suggest that the growth of RefSeq over time has strongly influenced the accuracy of k-mer based classification and profiling methods, resulting in different classification results depending on the particular database used. These results suggest a need for new algorithms specially adapted for large genome collections and better measures of classification uncertainty.},
	language = {en},
	urldate = {2018-08-10},
	journal = {bioRxiv},
	author = {Nasko, Daniel J. and Koren, Sergey and Phillippy, Adam M. and Treangen, Todd J.},
	month = apr,
	year = {2018},
	pages = {304972},
	file = {Full Text PDF:/home/simone/Zotero/storage/VE96TTGG/Nasko et al. - 2018 - RefSeq database growth influences the accuracy of .pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/NE5MV3RA/304972.html:text/html}
}

@article{trapnell_differential_2012,
	title = {Differential gene and transcript expression analysis of {RNA}-seq experiments with {TopHat} and {Cufflinks}},
	volume = {7},
	copyright = {2012 Nature Publishing Group},
	issn = {1750-2799},
	url = {https://www.nature.com/articles/nprot.2012.016},
	doi = {10.1038/nprot.2012.016},
	abstract = {Recent advances in high-throughput cDNA sequencing (RNA-seq) can reveal new genes and splice variants and quantify expression genome-wide in a single assay. The volume and complexity of data from RNA-seq experiments necessitate scalable, fast and mathematically principled analysis software. TopHat and Cufflinks are free, open-source software tools for gene discovery and comprehensive expression analysis of high-throughput mRNA sequencing (RNA-seq) data. Together, they allow biologists to identify new genes and new splice variants of known ones, as well as compare gene and transcript expression under two or more conditions. This protocol describes in detail how to use TopHat and Cufflinks to perform such analyses. It also covers several accessory tools and utilities that aid in managing data, including CummeRbund, a tool for visualizing RNA-seq analysis results. Although the procedure assumes basic informatics skills, these tools assume little to no background with RNA-seq analysis and are meant for novices and experts alike. The protocol begins with raw sequencing reads and produces a transcriptome assembly, lists of differentially expressed and regulated genes and transcripts, and publication-quality visualizations of analysis results. The protocol's execution time depends on the volume of transcriptome sequencing data and available computing resources but takes less than 1 d of computer time for typical experiments and ∼1 h of hands-on time.},
	language = {en},
	number = {3},
	urldate = {2018-08-10},
	journal = {Nature Protocols},
	author = {Trapnell, Cole and Roberts, Adam and Goff, Loyal and Pertea, Geo and Kim, Daehwan and Kelley, David R. and Pimentel, Harold and Salzberg, Steven L. and Rinn, John L. and Pachter, Lior},
	month = mar,
	year = {2012},
	pages = {562--578},
	file = {Snapshot:/home/simone/Zotero/storage/LNVI6T53/nprot.2012.html:text/html;Trapnell et al. - 2012 - Differential gene and transcript expression analys.pdf:/home/simone/Zotero/storage/WVUEGI5W/Trapnell et al. - 2012 - Differential gene and transcript expression analys.pdf:application/pdf}
}

@article{zou_regularization_2005,
	title = {Regularization and variable selection via the elastic net},
	issn = {1467-9868},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2005.00503.x%4010.1111/%28ISSN%291467-9868.TOP_SERIES_B_RESEARCH},
	doi = {10.1111/j.1467-9868.2005.00503.x@10.1111/(ISSN)1467-9868.TOP_SERIES_B_RESEARCH},
	abstract = {Summary. We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p≫n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
	language = {en},
	urldate = {2018-08-10},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Zou, Hui and Hastie, Trevor},
	month = apr,
	year = {2005},
	keywords = {Grouping effect, LARS algorithm, Lasso, p≫n problem, Penalization, Variable selection},
	pages = {301--320},
	file = {Snapshot:/home/simone/Zotero/storage/2KS9JXBL/(ISSN)1467-9868.html:text/html;Zou and Hastie - 2005 - Regularization and variable selection via the elas.pdf:/home/simone/Zotero/storage/WRSZUGJA/Zou and Hastie - 2005 - Regularization and variable selection via the elas.pdf:application/pdf}
}

@article{li_aligning_2013,
	title = {Aligning sequence reads, clone sequences and assembly contigs with {BWA}-{MEM}},
	url = {http://arxiv.org/abs/1303.3997},
	abstract = {Summary: BWA-MEM is a new alignment algorithm for aligning sequence reads or long query sequences against a large reference genome such as human. It automatically chooses between local and end-to-end alignments, supports paired-end reads and performs chimeric alignment. The algorithm is robust to sequencing errors and applicable to a wide range of sequence lengths from 70bp to a few megabases. For mapping 100bp sequences, BWA-MEM shows better performance than several state-of-art read aligners to date. Availability and implementation: BWA-MEM is implemented as a component of BWA, which is available at http://github.com/lh3/bwa. Contact: hengli@broadinstitute.org},
	urldate = {2018-08-10},
	journal = {arXiv:1303.3997 [q-bio]},
	author = {Li, Heng},
	month = mar,
	year = {2013},
	note = {arXiv: 1303.3997},
	keywords = {Quantitative Biology - Genomics},
	file = {arXiv\:1303.3997 PDF:/home/simone/Zotero/storage/RMKAGPMQ/Li - 2013 - Aligning sequence reads, clone sequences and assem.pdf:application/pdf;arXiv.org Snapshot:/home/simone/Zotero/storage/EVVIM7YQ/1303.html:text/html}
}

@article{zeller_potential_2014,
	title = {Potential of fecal microbiota for early‐stage detection of colorectal cancer},
	volume = {10},
	copyright = {© 2014 The Authors. Published under the terms of the CC BY 4.0 license. This is an open access article under the terms of the Creative Commons Attribution 4.0 License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited.},
	issn = {1744-4292, 1744-4292},
	url = {http://msb.embopress.org/content/10/11/766},
	doi = {10.15252/msb.20145645},
	abstract = {Several bacterial species have been implicated in the development of colorectal carcinoma (CRC), but CRC‐associated changes of fecal microbiota and their potential for cancer screening remain to be explored. Here, we used metagenomic sequencing of fecal samples to identify taxonomic markers that distinguished CRC patients from tumor‐free controls in a study population of 156 participants. Accuracy of metagenomic CRC detection was similar to the standard fecal occult blood test (FOBT) and when both approaches were combined, sensitivity improved {\textgreater} 45\% relative to the FOBT, while maintaining its specificity. Accuracy of metagenomic CRC detection did not differ significantly between early‐ and late‐stage cancer and could be validated in independent patient and control populations (N = 335) from different countries. CRC‐associated changes in the fecal microbiome at least partially reflected microbial community composition at the tumor itself, indicating that observed gene pool differences may reveal tumor‐related host–microbe interactions. Indeed, we deduced a metabolic shift from fiber degradation in controls to utilization of host carbohydrates and amino acids in CRC patients, accompanied by an increase of lipopolysaccharide metabolism.
Synopsis

{\textless}img class="highwire-embed" alt="Embedded Image" src="http://msb.embopress.org/sites/default/files/highwire/msb/10/11/766/embed/graphic-1.gif"/{\textgreater}

Metagenomic profiling of fecal samples from colorectal cancer (CRC) patients in comparison with tumor‐free controls reveals strong associations between the gut microbiota and cancer. Their potential for noninvasive cancer screening is explored systematically.

A classification model based on gut microbial marker species distinguishes CRC patients from controls with similar accuracy as the fecal occult blood test (FOBT), routinely used for clinical screening.Combining metagenomic data with the FOBT leads to a relative improvement in sensitivity of {\textgreater} 45\% over the FOBT alone at identical specificity.Detection accuracy of the metagenomic test is maintained in an independent study population and is still high for alternative microbiome readouts, such as the abundance of 16S rRNA OTUs or families of functionally related genes.Functional metagenomic analysis indicates an increased potential of CRC‐associated microbiota for degradation of host glycans and amino acids and for pro‐inflammatory lipopolysaccharide metabolism.},
	language = {en},
	number = {11},
	urldate = {2018-08-10},
	journal = {Molecular Systems Biology},
	author = {Zeller, Georg and Tap, Julien and Voigt, Anita Y. and Sunagawa, Shinichi and Kultima, Jens Roat and Costea, Paul I. and Amiot, Aurélien and Böhm, Jürgen and Brunetti, Francesco and Habermann, Nina and Hercog, Rajna and Koch, Moritz and Luciani, Alain and Mende, Daniel R. and Schneider, Martin A. and Schrotz‐King, Petra and Tournigand, Christophe and Nhieu, Jeanne Tran Van and Yamada, Takuji and Zimmermann, Jürgen and Benes, Vladimir and Kloor, Matthias and Ulrich, Cornelia M. and Doeberitz, Magnus von Knebel and Sobhani, Iradj and Bork, Peer},
	month = nov,
	year = {2014},
	pmid = {25432777},
	keywords = {cancer screening, colorectal cancer, fecal biomarkers, human gut microbiome, metagenomics},
	pages = {766},
	file = {Full Text PDF:/home/simone/Zotero/storage/R68ZCWF3/Zeller et al. - 2014 - Potential of fecal microbiota for early‐stage dete.pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/MEPEDGFP/766.html:text/html}
}

@article{breitwieser_review_nodate,
	title = {A review of methods and databases for metagenomic classification and assembly},
	url = {https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbx120/4210288},
	doi = {10.1093/bib/bbx120},
	abstract = {Abstract.  Microbiome research has grown rapidly over the past decade, with a proliferation of new methods that seek to make sense of large, complex data sets.},
	language = {en},
	urldate = {2018-08-10},
	journal = {Briefings in Bioinformatics},
	author = {Breitwieser, Florian P. and Lu, Jennifer and Salzberg, Steven L.},
	file = {Breitwieser et al. - A review of methods and databases for metagenomic .pdf:/home/simone/Zotero/storage/V8B7CKEW/Breitwieser et al. - A review of methods and databases for metagenomic .pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/IAVKV55T/4210288.html:text/html}
}

@article{segata_metagenomic_2012,
	title = {Metagenomic microbial community profiling using unique clade-specific marker genes},
	volume = {9},
	copyright = {2012 Nature Publishing Group},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.2066},
	doi = {10.1038/nmeth.2066},
	abstract = {Metagenomic shotgun sequencing data can identify microbes populating a microbial community and their proportions, but existing taxonomic profiling methods are inefficient for increasingly large data sets. We present an approach that uses clade-specific marker genes to unambiguously assign reads to microbial clades more accurately and {\textgreater}50× faster than current approaches. We validated our metagenomic phylogenetic analysis tool, MetaPhlAn, on terabases of short reads and provide the largest metagenomic profiling to date of the human gut. It can be accessed at http://huttenhower.sph.harvard.edu/metaphlan/.},
	language = {en},
	number = {8},
	urldate = {2018-08-10},
	journal = {Nature Methods},
	author = {Segata, Nicola and Waldron, Levi and Ballarini, Annalisa and Narasimhan, Vagheesh and Jousson, Olivier and Huttenhower, Curtis},
	month = aug,
	year = {2012},
	pages = {811--814},
	file = {Segata et al. - 2012 - Metagenomic microbial community profiling using un.pdf:/home/simone/Zotero/storage/6SLIZWLS/Segata et al. - 2012 - Metagenomic microbial community profiling using un.pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/YGS7Z778/nmeth.html:text/html}
}

@article{segata_phylophlan_2013,
	title = {{PhyloPhlAn} is a new method for improved phylogenetic and taxonomic placement of microbes},
	volume = {4},
	copyright = {2013 Nature Publishing Group},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/ncomms3304},
	doi = {10.1038/ncomms3304},
	abstract = {New microbial genomes are constantly being sequenced, and it is crucial to accurately determine their taxonomic identities and evolutionary relationships. Here we report PhyloPhlAn, a new method to assign microbial phylogeny and putative taxonomy using {\textgreater}400 proteins optimized from among 3,737 genomes. This method measures the sequence diversity of all clades, classifies genomes from deep-branching candidate divisions through closely related subspecies and improves consistency between phylogenetic and taxonomic groupings. PhyloPhlAn improved taxonomic accuracy for existing and newly sequenced genomes, detecting 157 erroneous labels, correcting 46 and placing or refining 130 new genomes. We provide examples of accurate classifications from subspecies (Sulfolobus spp.) to phyla, and of preliminary rooting of deep-branching candidate divisions, including consistent statistical support for Caldiserica (formerly candidate division OP5). PhyloPhlAn will thus be useful for both phylogenetic assessment and taxonomic quality control of newly sequenced genomes. The final phylogenies, conserved protein sequences and open-source implementation are available online.},
	language = {en},
	urldate = {2018-08-10},
	journal = {Nature Communications},
	author = {Segata, Nicola and Börnigen, Daniela and Morgan, Xochitl C. and Huttenhower, Curtis},
	month = aug,
	year = {2013},
	pages = {2304},
	file = {Full Text PDF:/home/simone/Zotero/storage/GHE87BE2/Segata et al. - 2013 - PhyloPhlAn is a new method for improved phylogenet.pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/QRBKNBN7/ncomms3304.html:text/html}
}

@article{eloe-fadrosh_metagenomics_2016,
	title = {Metagenomics uncovers gaps in amplicon-based detection of microbial diversity},
	volume = {1},
	copyright = {2016 Nature Publishing Group},
	issn = {2058-5276},
	url = {https://www.nature.com/articles/nmicrobiol201532},
	doi = {10.1038/nmicrobiol.2015.32},
	abstract = {Our view of microbial diversity has expanded greatly over the past 40 years, primarily through the wide application of PCR-based surveys of the small-subunit ribosomal RNA (SSU rRNA) gene. Yet significant gaps in knowledge remain due to well-recognized limitations of this method. Here, we systematically survey primer fidelity in SSU rRNA gene sequences recovered from over 6,000 assembled metagenomes sampled globally. Our findings show that approximately 10\% of environmental microbial sequences might be missed from classical PCR-based SSU rRNA gene surveys, mostly members of the Candidate Phyla Radiation (CPR) and as yet uncharacterized Archaea. These results underscore the extent of uncharacterized microbial diversity and provide fruitful avenues for describing additional phylogenetic lineages.},
	language = {en},
	number = {4},
	urldate = {2018-08-10},
	journal = {Nature Microbiology},
	author = {Eloe-Fadrosh, Emiley A. and Ivanova, Natalia N. and Woyke, Tanja and Kyrpides, Nikos C.},
	month = apr,
	year = {2016},
	pages = {15032},
	file = {Eloe-Fadrosh et al. - 2016 - Metagenomics uncovers gaps in amplicon-based detec.pdf:/home/simone/Zotero/storage/VJ9B2FXW/Eloe-Fadrosh et al. - 2016 - Metagenomics uncovers gaps in amplicon-based detec.pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/UEUI733N/nmicrobiol201532.html:text/html}
}

@article{wooley_primer_2010,
	title = {A {Primer} on {Metagenomics}},
	volume = {6},
	issn = {1553-7358},
	url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000667},
	doi = {10.1371/journal.pcbi.1000667},
	abstract = {Metagenomics is a discipline that enables the genomic study of uncultured microorganisms. Faster, cheaper sequencing technologies and the ability to sequence uncultured microbes sampled directly from their habitats are expanding and transforming our view of the microbial world. Distilling meaningful information from the millions of new genomic sequences presents a serious challenge to bioinformaticians. In cultured microbes, the genomic data come from a single clone, making sequence assembly and annotation tractable. In metagenomics, the data come from heterogeneous microbial communities, sometimes containing more than 10,000 species, with the sequence data being noisy and partial. From sampling, to assembly, to gene calling and function prediction, bioinformatics faces new demands in interpreting voluminous, noisy, and often partial sequence data. Although metagenomics is a relative newcomer to science, the past few years have seen an explosion in computational methods applied to metagenomic-based research. It is therefore not within the scope of this article to provide an exhaustive review. Rather, we provide here a concise yet comprehensive introduction to the current computational requirements presented by metagenomics, and review the recent progress made. We also note whether there is software that implements any of the methods presented here, and briefly review its utility. Nevertheless, it would be useful if readers of this article would avail themselves of the comment section provided by this journal, and relate their own experiences. Finally, the last section of this article provides a few representative studies illustrating different facets of recent scientific discoveries made using metagenomics.},
	language = {en},
	number = {2},
	urldate = {2018-08-13},
	journal = {PLOS Computational Biology},
	author = {Wooley, John C. and Godzik, Adam and Friedberg, Iddo},
	month = feb,
	year = {2010},
	keywords = {Metagenomics, Sequence assembly tools, Comparative genomics, Gene prediction, Genomic libraries, Genomic medicine, Genomics, Sequence databases},
	pages = {e1000667},
	file = {Full Text PDF:/home/simone/Zotero/storage/35XUM4B7/Wooley et al. - 2010 - A Primer on Metagenomics.pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/2NUVII5I/article.html:text/html}
}

@article{kucherov_algorithms_2018,
	title = {Algorithms for biosequence search: past, present and future},
	shorttitle = {Algorithms for biosequence search},
	url = {http://arxiv.org/abs/1808.01038},
	abstract = {The paper surveys the evolution of main algorithmic ideas used to compare and search biological sequences, including current trends and future prospects.},
	urldate = {2018-08-15},
	journal = {arXiv:1808.01038 [q-bio]},
	author = {Kucherov, Gregory},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.01038},
	keywords = {Quantitative Biology - Genomics},
	file = {arXiv\:1808.01038 PDF:/home/simone/Zotero/storage/FEE28QLV/Kucherov - 2018 - Algorithms for biosequence search past, present a.pdf:application/pdf;arXiv.org Snapshot:/home/simone/Zotero/storage/AC6UATAN/1808.html:text/html}
}

@article{magi_bioinformatics_2010,
	title = {Bioinformatics for {Next} {Generation} {Sequencing} {Data}},
	volume = {1},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {http://www.mdpi.com/2073-4425/1/2/294},
	doi = {10.3390/genes1020294},
	abstract = {The emergence of next-generation sequencing (NGS) platforms imposes increasing demands on statistical methods and bioinformatic tools for the analysis and the management of the huge amounts of data generated by these technologies. Even at the early stages of their commercial availability, a large number of softwares already exist for analyzing NGS data. These tools can be fit into many general categories including alignment of sequence reads to a reference, base-calling and/or polymorphism detection, de novo assembly from paired or unpaired reads, structural variant detection and genome browsing. This manuscript aims to guide readers in the choice of the available computational tools that can be used to face the several steps of the data analysis workflow.},
	language = {en},
	number = {2},
	urldate = {2018-08-15},
	journal = {Genes},
	author = {Magi, Alberto and Benelli, Matteo and Gozzini, Alessia and Girolami, Francesca and Torricelli, Francesca and Brandi, Maria Luisa and Magi, Alberto and Benelli, Matteo and Gozzini, Alessia and Girolami, Francesca and Torricelli, Francesca and Brandi, Maria Luisa},
	month = sep,
	year = {2010},
	keywords = {bioinformatics, data analysis, sequencing},
	pages = {294--307},
	file = {Full Text PDF:/home/simone/Zotero/storage/B36IZAUH/Magi et al. - 2010 - Bioinformatics for Next Generation Sequencing Data.pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/CHZNVRN3/htm.html:text/html}
}

@article{kulski_next-generation_2016,
	title = {Next-{Generation} {Sequencing} — {An} {Overview} of the {History}, {Tools}, and “{Omic}” {Applications}},
	url = {https://www.intechopen.com/books/next-generation-sequencing-advances-applications-and-challenges/next-generation-sequencing-an-overview-of-the-history-tools-and-omic-applications},
	doi = {10.5772/61964},
	language = {en},
	urldate = {2018-08-15},
	journal = {Next Generation Sequencing - Advances, Applications and Challenges},
	author = {Kulski, Jerzy K.},
	year = {2016},
	file = {Full Text PDF:/home/simone/Zotero/storage/ZTQXSVJA/Kulski - 2016 - Next-Generation Sequencing — An Overview of the Hi.pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/UDCBHP7R/next-generation-sequencing-an-overview-of-the-history-tools-and-omic-applications.html:text/html}
}

@incollection{bragg_metagenomics_2014,
	series = {Methods in {Molecular} {Biology}},
	title = {Metagenomics {Using} {Next}-{Generation} {Sequencing}},
	isbn = {978-1-62703-711-2 978-1-62703-712-9},
	url = {http://link.springer.com/protocol/10.1007/978-1-62703-712-9_15},
	abstract = {Traditionally, microbial genome sequencing has been restricted to the small number of species that can be grown in pure culture [1]. The progressive development of culture-independent methods over the last 15 years now allows researchers to sequence microbial communities directly from environmental samples. This approach is commonly referred to as “metagenomics” or “community genomics”. However, the term metagenomics is applied liberally in the literature to describe any culture-independent analysis of microbial communities. Here, we define metagenomics as shotgun (“random”) sequencing of the genomic DNA of a sample taken directly from the environment. The metagenome can be thought of as a sampling of the collective genome of the microbial community. We outline the considerations and analyses that should be undertaken to ensure the success of a metagenomic sequencing project, including the choice of sequencing platform and methods for assembly, binning, annotation, and comparative analysis.},
	language = {en},
	urldate = {2018-08-15},
	booktitle = {Environmental {Microbiology}},
	publisher = {Humana Press, Totowa, NJ},
	author = {Bragg, Lauren and Tyson, Gene W.},
	year = {2014},
	doi = {10.1007/978-1-62703-712-9_15},
	pages = {183--201},
	file = {Full Text PDF:/home/simone/Zotero/storage/3ARZ67EY/Bragg and Tyson - 2014 - Metagenomics Using Next-Generation Sequencing.pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/TNTLISPJ/10.html:text/html}
}

@article{huson_megan_2007,
	title = {{MEGAN} analysis of metagenomic data},
	volume = {17},
	issn = {1088-9051, 1549-5469},
	url = {http://genome.cshlp.org/content/early/2007/01/01/gr.5969107},
	doi = {10.1101/gr.5969107},
	abstract = {Metagenomics is the study of the genomic content of a sample of organisms obtained from a common habitat using targeted or random sequencing. Goals include understanding the extent and role of microbial diversity. The taxonomical content of such a sample is usually estimated by comparison against sequence databases of known sequences. Most published studies use the analysis of paired-end reads, complete sequences of environmental fosmid and BAC clones, or environmental assemblies. Emerging sequencing-by-synthesis technologies with very high throughput are paving the way to low-cost random “shotgun” approaches. This paper introduces MEGAN, a new computer program that allows laptop analysis of large metagenomic data sets. In a preprocessing step, the set of DNA sequences is compared against databases of known sequences using BLAST or another comparison tool. MEGAN is then used to compute and explore the taxonomical content of the data set, employing the NCBI taxonomy to summarize and order the results. A simple lowest common ancestor algorithm assigns reads to taxa such that the taxonomical level of the assigned taxon reflects the level of conservation of the sequence. The software allows large data sets to be dissected without the need for assembly or the targeting of specific phylogenetic markers. It provides graphical and statistical output for comparing different data sets. The approach is applied to several data sets, including the Sargasso Sea data set, a recently published metagenomic data set sampled from a mammoth bone, and several complete microbial genomes. Also, simulations that evaluate the performance of the approach for different read lengths are presented.},
	language = {en},
	number = {3},
	urldate = {2018-08-15},
	journal = {Genome Research},
	author = {Huson, Daniel H. and Auch, Alexander F. and Qi, Ji and Schuster, Stephan C.},
	month = jan,
	year = {2007},
	pmid = {17255551},
	pages = {000--000},
	file = {Full Text PDF:/home/simone/Zotero/storage/Z4EULGUT/Huson et al. - 2007 - MEGAN analysis of metagenomic data.pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/AZWIELKB/gr.5969107.html:text/html}
}

@article{garnier_complete_2003,
	title = {The complete genome sequence of {Mycobacterium} bovis},
	volume = {100},
	copyright = {Copyright © 2003, The National Academy of  Sciences},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/100/13/7877},
	doi = {10.1073/pnas.1130426100},
	abstract = {Mycobacterium bovis is the causative agent of tuberculosis in a range of animal species and man, with worldwide annual losses to agriculture of \$3 billion. The human burden of tuberculosis caused by the bovine tubercle bacillus is still largely unknown. M. bovis was also the progenitor for the M. bovis bacillus Calmette–Guérin vaccine strain, the most widely used human vaccine. Here we describe the 4,345,492-bp genome sequence of M. bovis AF2122/97 and its comparison with the genomes of Mycobacterium tuberculosis and Mycobacterium leprae. Strikingly, the genome sequence of M. bovis is {\textgreater}99.95\% identical to that of M. tuberculosis, but deletion of genetic information has led to a reduced genome size. Comparison with M. leprae reveals a number of common gene losses, suggesting the removal of functional redundancy. Cell wall components and secreted proteins show the greatest variation, indicating their potential role in host–bacillus interactions or immune evasion. Furthermore, there are no genes unique to M. bovis, implying that differential gene expression may be the key to the host tropisms of human and bovine bacilli. The genome sequence therefore offers major insight on the evolution, host preference, and pathobiology of M. bovis.},
	language = {en},
	number = {13},
	urldate = {2018-08-15},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Garnier, Thierry and Eiglmeier, Karin and Camus, Jean-Christophe and Medina, Nadine and Mansoor, Huma and Pryor, Melinda and Duthoy, Stephanie and Grondin, Sophie and Lacroix, Celine and Monsempe, Christel and Simon, Sylvie and Harris, Barbara and Atkin, Rebecca and Doggett, Jon and Mayes, Rebecca and Keating, Lisa and Wheeler, Paul R. and Parkhill, Julian and Barrell, Bart G. and Cole, Stewart T. and Gordon, Stephen V. and Hewinson, R. Glyn},
	month = jun,
	year = {2003},
	pmid = {12788972},
	pages = {7877--7882},
	file = {Full Text PDF:/home/simone/Zotero/storage/J7QWJJWQ/Garnier et al. - 2003 - The complete genome sequence of Mycobacterium bovi.pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/MJZA9Q34/7877.html:text/html}
}

@article{tibshirani_regression_1996,
	title = {Regression {Shrinkage} and {Selection} via the {Lasso}},
	volume = {58},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2346178},
	abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
	number = {1},
	urldate = {2018-08-16},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Tibshirani, Robert},
	year = {1996},
	pages = {267--288},
	file = {Tibshirani - 1996 - Regression Shrinkage and Selection via the Lasso.pdf:/home/simone/Zotero/storage/TLG82S5K/Tibshirani - 1996 - Regression Shrinkage and Selection via the Lasso.pdf:application/pdf}
}

@misc{freedman_statistical_2009,
	title = {Statistical {Models} by {David} {A}. {Freedman}},
	url = {/core/books/statistical-models/68F8872C7788AF62BD6513F7071EE1BA},
	abstract = {Cambridge Core - Statistical Theory and Methods - Statistical Models -  by David A. Freedman},
	language = {en},
	urldate = {2018-08-16},
	journal = {Cambridge Core},
	author = {Freedman, David A.},
	month = apr,
	year = {2009},
	doi = {10.1017/CBO9780511815867},
	file = {Snapshot:/home/simone/Zotero/storage/8MZIXS5A/68F8872C7788AF62BD6513F7071EE1BA.html:text/html}
}

@misc{noauthor_linear_2018,
	title = {Linear regression},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Linear_regression&oldid=851432903},
	abstract = {In statistics, linear regression is a linear approach to modelling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression. This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models. Most commonly, the conditional mean of the response given the values of the explanatory variables (or predictors) is assumed to be an affine function of those values; less commonly, the conditional median or some other quantile is used. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis.
Linear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.
Linear regression has many practical uses. Most applications fall into one of the following two broad categories:

If the goal is prediction, or forecasting, or error reduction, linear regression can be used to fit a predictive model to an observed data set of values of the response and explanatory variables. After developing such a model, if additional values of the explanatory variables are collected without an accompanying response value, the fitted model can be used to make a prediction of the response.
If the goal is to explain variation in the response variable that can be attributed to variation in the explanatory variables, linear regression analysis can be applied to quantify the strength of the relationship between the response and the explanatory variables, and in particular to determine whether some explanatory variables may have no linear relationship with the response at all, or to identify which subsets of explanatory variables may contain redundant information about the response.Linear regression models are often fitted using the least squares approach, but they may also be fitted in other ways, such as by minimizing the "lack of fit" in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares cost function as in ridge regression (L2-norm penalty) and lasso (L1-norm penalty). Conversely, the least squares approach can be used to fit models that are not linear models. Thus, although the terms "least squares" and "linear model" are closely linked, they are not synonymous.},
	language = {en},
	urldate = {2018-08-16},
	journal = {Wikipedia},
	month = jul,
	year = {2018},
	note = {Page Version ID: 851432903},
	file = {Snapshot:/home/simone/Zotero/storage/D8M85DTD/index.html:text/html}
}

@misc{noauthor_coordinate_2018,
	title = {Coordinate descent},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Coordinate_descent&oldid=846517541},
	abstract = {Coordinate descent is an optimization algorithm that successively minimizes along coordinate directions to find the minimum of a function. At each iteration, the algorithm determines a coordinate or coordinate block via a coordinate selection rule, then exactly or inexactly minimizes over the corresponding coordinate hyperplane while fixing all other coordinates or coordinate blocks. A line search along the coordinate direction can be performed at the current iterate to determine the appropriate step size. Coordinate descent is applicable in both differentiable and derivative-free contexts.},
	language = {en},
	urldate = {2018-08-16},
	journal = {Wikipedia},
	month = jun,
	year = {2018},
	note = {Page Version ID: 846517541},
	file = {Snapshot:/home/simone/Zotero/storage/4W294HTB/index.html:text/html}
}

@misc{noauthor_general_2018,
	title = {General linear model},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=General_linear_model&oldid=850020279},
	abstract = {The general linear model or multivariate regression model is a statistical linear model. It may be written as

  
    
      
        
          Y
        
        =
        
          X
        
        
          B
        
        +
        
          U
        
        ,
      
    
    \{{\textbackslash}displaystyle {\textbackslash}mathbf \{Y\} ={\textbackslash}mathbf \{X\} {\textbackslash}mathbf \{B\} +{\textbackslash}mathbf \{U\} ,\}
  where Y is a matrix with series of multivariate measurements (each column being a set of measurements on one of the dependent variables), X is a matrix of observations on independent variables that might be a design matrix (each column being a set of observations on one of the independent variables), B is a matrix containing parameters that are usually to be estimated and U is a matrix containing errors (noise).
The errors are usually assumed to be uncorrelated across measurements, and follow a multivariate normal distribution.  If the errors do not follow a multivariate normal distribution, generalized linear models may be used to relax assumptions about Y and U.
The general linear model incorporates a number of different statistical models: ANOVA, ANCOVA, MANOVA, MANCOVA, ordinary linear regression, t-test and F-test. The general linear model is a generalization of multiple linear regression model to the case of more than one dependent variable. If Y, B, and U were column vectors, the matrix equation above would represent multiple linear regression.
Hypothesis tests with the general linear model can be made in two ways: multivariate or as several independent univariate tests. In multivariate tests the columns of Y are tested together, whereas in univariate tests the columns of Y are tested independently, i.e., as multiple univariate tests with the same design matrix.},
	language = {en},
	urldate = {2018-08-16},
	journal = {Wikipedia},
	month = jul,
	year = {2018},
	note = {Page Version ID: 850020279},
	file = {Snapshot:/home/simone/Zotero/storage/A7MQDEVC/index.html:text/html}
}

@misc{noauthor_generalized_2018,
	title = {Generalized linear model},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Generalized_linear_model&oldid=846910075},
	abstract = {In statistics, the generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution.  The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.
Generalized linear models were formulated by John Nelder and Robert Wedderburn as a way of unifying various other statistical models, including linear regression, logistic regression and Poisson regression. They proposed an iteratively reweighted least squares method for maximum likelihood estimation of the model parameters. Maximum-likelihood estimation remains popular and is the default method on many statistical computing packages.  Other approaches, including Bayesian approaches and least squares fits to variance stabilized responses, have been developed.},
	language = {en},
	urldate = {2018-08-16},
	journal = {Wikipedia},
	month = jun,
	year = {2018},
	note = {Page Version ID: 846910075},
	file = {Snapshot:/home/simone/Zotero/storage/6JVD5WEE/index.html:text/html}
}

@misc{noauthor_ordinary_2018,
	title = {Ordinary least squares},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Ordinary_least_squares&oldid=855225783},
	abstract = {In statistics, ordinary least squares (OLS) is a type of linear least squares method for estimating the unknown parameters in a linear regression model. OLS chooses the parameters of a linear function of a set of explanatory variables by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being predicted) in the given dataset and those predicted by the linear function. 
Geometrically, this is seen as the sum of the squared distances, parallel to the axis of the dependent variable, between each data point in the set and the corresponding point on the regression surface – the smaller the differences, the better the model fits the data. The resulting estimator can be expressed by a simple formula, especially in the case of a simple linear regression, in which there is a single regressor on the right side of the regression equation.
The OLS estimator is consistent when the regressors are exogenous, and optimal in the class of linear unbiased estimators when the errors are homoscedastic and serially uncorrelated. Under these conditions, the method of OLS provides minimum-variance mean-unbiased estimation when the errors have finite variances. Under the additional assumption that the errors are normally distributed, OLS is the maximum likelihood estimator.
OLS is used in fields as diverse as economics (econometrics), political science, psychology and engineering (control theory and signal processing).},
	language = {en},
	urldate = {2018-08-20},
	journal = {Wikipedia},
	month = aug,
	year = {2018},
	note = {Page Version ID: 855225783},
	file = {Snapshot:/home/simone/Zotero/storage/CA2ZQ57J/index.html:text/html}
}

@article{waldron_optimized_2011,
	title = {Optimized application of penalized regression methods to diverse genomic data},
	volume = {27},
	issn = {1367-4803},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3232376/},
	doi = {10.1093/bioinformatics/btr591},
	abstract = {Motivation: Penalized regression methods have been adopted widely for high-dimensional feature selection and prediction in many bioinformatic and biostatistical contexts. While their theoretical properties are well-understood, specific methodology for their optimal application to genomic data has not been determined., Results: Through simulation of contrasting scenarios of correlated high-dimensional survival data, we compared the LASSO, Ridge and Elastic Net penalties for prediction and variable selection. We found that a 2D tuning of the Elastic Net penalties was necessary to avoid mimicking the performance of LASSO or Ridge regression. Furthermore, we found that in a simulated scenario favoring the LASSO penalty, a univariate pre-filter made the Elastic Net behave more like Ridge regression, which was detrimental to prediction performance. We demonstrate the real-life application of these methods to predicting the survival of cancer patients from microarray data, and to classification of obese and lean individuals from metagenomic data. Based on these results, we provide an optimized set of guidelines for the application of penalized regression for reproducible class comparison and prediction with genomic data., Availability and Implementation: A parallelized implementation of the methods presented for regression and for simulation of synthetic data is provided as the pensim R package, available at http://cran.r-project.org/web/packages/pensim/index.html., Contact: chuttenh@hsph.harvard.edu; juris@ai.utoronto.ca, Supplementary Information: Supplementary data are available at Bioinformatics online.},
	number = {24},
	urldate = {2018-08-20},
	journal = {Bioinformatics},
	author = {Waldron, Levi and Pintilie, Melania and Tsao, Ming-Sound and Shepherd, Frances A. and Huttenhower, Curtis and Jurisica, Igor},
	month = dec,
	year = {2011},
	pmid = {22156367},
	pmcid = {PMC3232376},
	pages = {3399--3406},
	file = {PubMed Central Full Text PDF:/home/simone/Zotero/storage/MYDKJ53U/Waldron et al. - 2011 - Optimized application of penalized regression meth.pdf:application/pdf}
}

@article{johnson_better_2014,
	title = {A better sequence-read simulator program for metagenomics},
	volume = {15},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/1471-2105-15-S9-S14},
	doi = {10.1186/1471-2105-15-S9-S14},
	abstract = {There are many programs available for generating simulated whole-genome shotgun sequence reads. The data generated by many of these programs follow predefined models, which limits their use to the authors' original intentions. For example, many models assume that read lengths follow a uniform or normal distribution. Other programs generate models from actual sequencing data, but are limited to reads from single-genome studies. To our knowledge, there are no programs that allow a user to generate simulated data following non-parametric read-length distributions and quality profiles based on empirically-derived information from metagenomics sequencing data.},
	number = {9},
	urldate = {2018-08-20},
	journal = {BMC Bioinformatics},
	author = {Johnson, Stephen and Trost, Brett and Long, Jeffrey R. and Pittet, Vanessa and Kusalik, Anthony},
	month = sep,
	year = {2014},
	pages = {S14},
	file = {Full Text PDF:/home/simone/Zotero/storage/NI8679PL/Johnson et al. - 2014 - A better sequence-read simulator program for metag.pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/MPK3A8Y2/1471-2105-15-S9-S14.html:text/html}
}

@article{jia_nessm:_2013,
	title = {{NeSSM}: {A} {Next}-{Generation} {Sequencing} {Simulator} for {Metagenomics}},
	volume = {8},
	issn = {1932-6203},
	shorttitle = {{NeSSM}},
	url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0075448},
	doi = {10.1371/journal.pone.0075448},
	abstract = {Background Metagenomics can reveal the vast majority of microbes that have been missed by traditional cultivation-based methods. Due to its extremely wide range of application areas, fast metagenome sequencing simulation systems with high fidelity are in great demand to facilitate the development and comparison of metagenomics analysis tools. Results We present here a customizable metagenome simulation system: NeSSM (Next-generation Sequencing Simulator for Metagenomics). Combining complete genomes currently available, a community composition table, and sequencing parameters, it can simulate metagenome sequencing better than existing systems. Sequencing error models based on the explicit distribution of errors at each base and sequencing coverage bias are incorporated in the simulation. In order to improve the fidelity of simulation, tools are provided by NeSSM to estimate the sequencing error models, sequencing coverage bias and the community composition directly from existing metagenome sequencing data. Currently, NeSSM supports single-end and pair-end sequencing for both 454 and Illumina platforms. In addition, a GPU (graphics processing units) version of NeSSM is also developed to accelerate the simulation. By comparing the simulated sequencing data from NeSSM with experimental metagenome sequencing data, we have demonstrated that NeSSM performs better in many aspects than existing popular metagenome simulators, such as MetaSim, GemSIM and Grinder. The GPU version of NeSSM is more than one-order of magnitude faster than MetaSim. Conclusions NeSSM is a fast simulation system for high-throughput metagenome sequencing. It can be helpful to develop tools and evaluate strategies for metagenomics analysis and it’s freely available for academic users at http://cbb.sjtu.edu.cn/{\textasciitilde}ccwei/pub/software/NeSSM.php.},
	language = {en},
	number = {10},
	urldate = {2018-08-20},
	journal = {PLOS ONE},
	author = {Jia, Ben and Xuan, Liming and Cai, Kaiye and Hu, Zhiqiang and Ma, Liangxiao and Wei, Chaochun},
	month = oct,
	year = {2013},
	keywords = {Genome analysis, Genome sequencing, Metagenomics, Genomics, Genomic databases, Next-generation sequencing, Shotgun sequencing, Simulation and modeling},
	pages = {e75448},
	file = {Full Text PDF:/home/simone/Zotero/storage/WBVJXMHA/Jia et al. - 2013 - NeSSM A Next-Generation Sequencing Simulator for .pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/6D68J994/article.html:text/html}
}

@article{donoho_compressed_2006,
	title = {Compressed sensing},
	volume = {52},
	issn = {0018-9448},
	url = {http://ieeexplore.ieee.org/document/1614066/},
	doi = {10.1109/TIT.2006.871582},
	abstract = {Suppose is an unknown vector in (a digital image or signal); we plan to measure general linear functionals of and then reconstruct. If is known to be compressible by transform coding with a known transform, and we reconstruct via the nonlinear procedure deﬁned here, the number of measurements can be dramatically smaller than the size . Thus, certain natural classes of images with pixels need only = ( 1 4 log5 2( )) nonadaptive nonpixel samples for faithful recovery, as opposed to the usual pixel samples.},
	language = {en},
	number = {4},
	urldate = {2018-08-20},
	journal = {IEEE Transactions on Information Theory},
	author = {Donoho, D.L.},
	month = apr,
	year = {2006},
	pages = {1289--1306},
	file = {Donoho - 2006 - Compressed sensing.pdf:/home/simone/Zotero/storage/7BHJA82K/Donoho - 2006 - Compressed sensing.pdf:application/pdf}
}

@article{breiman_heuristics_1996,
	title = {Heuristics of instability and stabilization in model selection},
	volume = {24},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1032181158},
	doi = {10.1214/aos/1032181158},
	abstract = {In model selection, usually a "best" predictor is chosen from a collection μ{\textasciicircum}(⋅,s)μ{\textasciicircum}(⋅,s)\{{\textbackslash}hat\{{\textbackslash}mu\}({\textbackslash}cdot, s)\} of predictors where μ{\textasciicircum}(⋅,s)μ{\textasciicircum}(⋅,s){\textbackslash}hat\{{\textbackslash}mu\}({\textbackslash}cdot, s) is the minimum least-squares predictor in a collection UsUs{\textbackslash}mathsf\{U\}\_s of predictors. Here s is a complexity parameter; that is, the smaller s, the lower dimensional/smoother the models in UsUs{\textbackslash}mathsf\{U\}\_s. If LL{\textbackslash}mathsf\{L\} is the data used to derive the sequence μ{\textasciicircum}(⋅,s)μ{\textasciicircum}(⋅,s)\{{\textbackslash}hat\{{\textbackslash}mu\}({\textbackslash}cdot, s)\}, the procedure is called unstable if a small change in LL{\textbackslash}mathsf\{L\} can cause large changes in μ{\textasciicircum}(⋅,s)μ{\textasciicircum}(⋅,s)\{{\textbackslash}hat\{{\textbackslash}mu\}({\textbackslash}cdot, s)\}. With a crystal ball, one could pick the predictor in μ{\textasciicircum}(⋅,s)μ{\textasciicircum}(⋅,s)\{{\textbackslash}hat\{{\textbackslash}mu\}({\textbackslash}cdot, s)\} having minimum prediction error. Without prescience, one uses test sets, cross-validation and so forth. The difference in prediction error between the crystal ball selection and the statistician's choice we call predictive loss. For an unstable procedure the predictive loss is large. This is shown by some analytics in a simple case and by simulation results in a more complex comparison of four different linear regression methods. Unstable procedures can be stabilized by perturbing the data, getting a new predictor sequence μ′{\textasciicircum}(⋅,s)μ′{\textasciicircum}(⋅,s)\{{\textbackslash}hat\{{\textbackslash}mu'\}({\textbackslash}cdot, s)\} and then averaging over many such predictor sequences.},
	language = {en},
	number = {6},
	urldate = {2018-08-20},
	journal = {The Annals of Statistics},
	author = {Breiman, Leo},
	month = dec,
	year = {1996},
	mrnumber = {MR1425957},
	zmnumber = {0867.62055},
	keywords = {cross-validation, prediction error, predictive loss, Regression, subset selection},
	pages = {2350--2383},
	file = {Full Text PDF:/home/simone/Zotero/storage/YDIMP8H8/Breiman - 1996 - Heuristics of instability and stabilization in mod.pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/VT7EH2PL/1032181158.html:text/html}
}

@article{wright_coordinate_2015,
	title = {Coordinate descent algorithms},
	volume = {151},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/s10107-015-0892-3},
	doi = {10.1007/s10107-015-0892-3},
	abstract = {Coordinate descent algorithms solve optimization problems by successively performing approximate minimization along coordinate directions or coordinate hyperplanes. They have been used in applications for many years, and their popularity continues to grow because of their usefulness in data analysis, machine learning, and other areas of current interest. This paper describes the fundamentals of the coordinate descent approach, together with variants and extensions and their convergence properties, mostly with reference to convex objectives. We pay particular attention to a certain problem structure that arises frequently in machine learning applications, showing that efficient implementations of accelerated coordinate descent algorithms are possible for problems of this type. We also present some parallel variants and discuss their convergence properties under several models of parallel execution.},
	language = {en},
	number = {1},
	urldate = {2018-08-20},
	journal = {Mathematical Programming},
	author = {Wright, Stephen J.},
	month = jun,
	year = {2015},
	keywords = {49M20, 90C25, Coordinate descent, Parallel numerical computing, Randomized algorithms},
	pages = {3--34},
	file = {Springer Full Text PDF:/home/simone/Zotero/storage/SIMX8KB4/Wright - 2015 - Coordinate descent algorithms.pdf:application/pdf}
}

@article{turaev_high_2016,
	series = {Systems biology • {Nanobiotechnology}},
	title = {High definition for systems biology of microbial communities: metagenomics gets genome-centric and strain-resolved},
	volume = {39},
	issn = {0958-1669},
	shorttitle = {High definition for systems biology of microbial communities},
	url = {http://www.sciencedirect.com/science/article/pii/S0958166916301197},
	doi = {10.1016/j.copbio.2016.04.011},
	abstract = {The systems biology of microbial communities, organismal communities inhabiting all ecological niches on earth, has in recent years been strongly facilitated by the rapid development of experimental, sequencing and data analysis methods. Novel experimental approaches and binning methods in metagenomics render the semi-automatic reconstructions of near-complete genomes of uncultivable bacteria possible, while advances in high-resolution amplicon analysis allow for efficient and less biased taxonomic community characterization. This will also facilitate predictive modeling approaches, hitherto limited by the low resolution of metagenomic data. In this review, we pinpoint the most promising current developments in metagenomics. They facilitate microbial systems biology towards a systemic understanding of mechanisms in microbial communities with scopes of application in many areas of our daily life.},
	urldate = {2018-08-21},
	journal = {Current Opinion in Biotechnology},
	author = {Turaev, Dmitrij and Rattei, Thomas},
	month = jun,
	year = {2016},
	pages = {174--181},
	file = {ScienceDirect Full Text PDF:/home/simone/Zotero/storage/ZJAHN9Q4/Turaev and Rattei - 2016 - High definition for systems biology of microbial c.pdf:application/pdf;ScienceDirect Snapshot:/home/simone/Zotero/storage/VR6ITMWL/S0958166916301197.html:text/html}
}

@article{peabody_evaluation_2015,
	title = {Evaluation of shotgun metagenomics sequence classification methods using in silico and in vitro simulated communities},
	volume = {16},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/s12859-015-0788-5},
	doi = {10.1186/s12859-015-0788-5},
	abstract = {BackgroundThe field of metagenomics (study of genetic material recovered directly from an environment) has grown rapidly, with many bioinformatics analysis methods being developed. To ensure appropriate use of such methods, robust comparative evaluation of their accuracy and features is needed. For taxonomic classification of sequence reads, such evaluation should include use of clade exclusion, which better evaluates a method’s accuracy when identical sequences are not present in any reference database, as is common in metagenomic analysis. To date, relatively small evaluations have been performed, with evaluation approaches like clade exclusion limited to assessment of new methods by the authors of the given method. What is needed is a rigorous, independent comparison between multiple major methods, using the same in silico and in vitro test datasets, with and without approaches like clade exclusion, to better characterize accuracy under different conditions.ResultsAn overview of the features of 38 bioinformatics methods is provided, evaluating accuracy with a focus on 11 programs that have reference databases that can be modified and therefore most robustly evaluated with clade exclusion. Taxonomic classification of sequence reads was evaluated using both in silico and in vitro mock bacterial communities. Clade exclusion was used at taxonomic levels from species to class—identifying how well methods perform in progressively more difficult scenarios. A wide range of variability was found in the sensitivity, precision, overall accuracy, and computational demand for the programs evaluated. In experiments where distilled water was spiked with only 11 bacterial species, frequently dozens to hundreds of species were falsely predicted by the most popular programs. The different features of each method (forces predictions or not, etc.) are summarized, and additional analysis considerations discussed.ConclusionsThe accuracy of shotgun metagenomics classification methods varies widely. No one program clearly outperformed others in all evaluation scenarios; rather, the results illustrate the strengths of different methods for different purposes. Researchers must appreciate method differences, choosing the program best suited for their particular analysis to avoid very misleading results. Use of standardized datasets for method comparisons is encouraged, as is use of mock microbial community controls suitable for a particular metagenomic analysis.},
	language = {en},
	number = {1},
	urldate = {2018-08-21},
	journal = {BMC Bioinformatics},
	author = {Peabody, Michael A. and Van Rossum, Thea and Lo, Raymond and Brinkman, Fiona S. L.},
	month = nov,
	year = {2015},
	keywords = {Metagenomics, Accuracy, Comparison, Evaluation, Taxonomic classification},
	pages = {362},
	file = {Springer Full Text PDF:/home/simone/Zotero/storage/JENR4V5L/Peabody et al. - 2015 - Evaluation of shotgun metagenomics sequence classi.pdf:application/pdf}
}

@article{altschul_basic_1990,
	title = {Basic local alignment search tool},
	volume = {215},
	issn = {0022-2836},
	doi = {10.1016/S0022-2836(05)80360-2},
	abstract = {A new approach to rapid sequence comparison, basic local alignment search tool (BLAST), directly approximates alignments that optimize a measure of local similarity, the maximal segment pair (MSP) score. Recent mathematical results on the stochastic properties of MSP scores allow an analysis of the performance of this method as well as the statistical significance of alignments it generates. The basic algorithm is simple and robust; it can be implemented in a number of ways and applied in a variety of contexts including straightforward DNA and protein sequence database searches, motif searches, gene identification searches, and in the analysis of multiple regions of similarity in long DNA sequences. In addition to its flexibility and tractability to mathematical analysis, BLAST is an order of magnitude faster than existing sequence comparison tools of comparable sensitivity.},
	language = {eng},
	number = {3},
	journal = {Journal of Molecular Biology},
	author = {Altschul, S. F. and Gish, W. and Miller, W. and Myers, E. W. and Lipman, D. J.},
	month = oct,
	year = {1990},
	pmid = {2231712},
	keywords = {Algorithms, Amino Acid Sequence, Base Sequence, Databases, Factual, Mutation, Sensitivity and Specificity, Sequence Homology, Nucleic Acid, Software},
	pages = {403--410}
}

@article{langmead_fast_2012,
	title = {Fast gapped-read alignment with {Bowtie} 2},
	volume = {9},
	copyright = {2012 Nature Publishing Group},
	issn = {1548-7105},
	url = {http://www.nature.com/articles/nmeth.1923},
	doi = {10.1038/nmeth.1923},
	abstract = {As the rate of sequencing increases, greater throughput is demanded from read aligners. The full-text minute index is often used to make alignment very fast and memory-efficient, but the approach is ill-suited to finding longer, gapped alignments. Bowtie 2 combines the strengths of the full-text minute index with the flexibility and speed of hardware-accelerated dynamic programming algorithms to achieve a combination of high speed, sensitivity and accuracy.},
	language = {en},
	number = {4},
	urldate = {2018-08-22},
	journal = {Nature Methods},
	author = {Langmead, Ben and Salzberg, Steven L.},
	month = apr,
	year = {2012},
	pages = {357--359},
	file = {Full Text PDF:/home/simone/Zotero/storage/NDTI6M65/Langmead and Salzberg - 2012 - Fast gapped-read alignment with Bowtie 2.pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/WEEBQI98/nmeth.html:text/html}
}

@article{dilthey_metamaps_2018-1,
	title = {{MetaMaps} - {Strain}-level metagenomic assignment and compositional estimation for long reads},
	copyright = {© 2018, Posted by Cold Spring Harbor Laboratory. This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available for use under a CC0 license},
	url = {https://www.biorxiv.org/content/early/2018/07/20/372474},
	doi = {10.1101/372474},
	abstract = {Metagenomic sequence classification should be fast, accurate and information-rich. Emerging long-read sequencing technologies promise to improve the balance between these factors but most existing methods were designed for short reads. MetaMaps is a new method, specifically developed for long reads, that combines the accuracy of slower alignment-based methods with the scalability of faster k-mer-based methods. Using an approximate mapping algorithm, it is capable of mapping a long-read metagenome to a comprehensive RefSeq database with {\textgreater}12,000 genomes in {\textless}30 GB or RAM on a laptop computer. Integrating these mappings with a probabilistic scoring scheme and EM-based estimation of sample composition, MetaMaps achieves {\textgreater}95\% accuracy for species-level read assignment and r2 {\textgreater} 0.98 for the estimation of sample composition on both simulated and real data. Uniquely, MetaMaps outputs mapping locations and qualities for all classified reads, enabling functional studies (e.g. gene presence/absence) and the detection of novel species not present in the current database.},
	language = {en},
	urldate = {2018-08-22},
	journal = {bioRxiv},
	author = {Dilthey, Alexander and Jain, Chirag and Koren, Sergey and Phillippy, Adam},
	month = jul,
	year = {2018},
	pages = {372474},
	file = {Full Text PDF:/home/simone/Zotero/storage/ETTXTN9C/Dilthey et al. - 2018 - MetaMaps - Strain-level metagenomic assignment and.pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/WDCG6LVW/372474.html:text/html}
}

@misc{brinda_prophyle:_2017,
	title = {{ProPhyle}: a phylogeny-based metagenomic classifier using the {Burrows}-{Wheeler} {Transform}},
	shorttitle = {{ProPhyle}},
	url = {https://zenodo.org/record/1045427},
	abstract = {Metagenomics is a powerful approach to study genetic content of environmental samples and it has been strongly promoted by Next-Generation Sequencing technologies. The aim of metagenomic classification is to assign each sequence of the metagenome to a corresponding taxonomic unit, or to classify it as “novel”. To cope with increasingly large metagenomic projects, researchers resort to alignment-free methods. The most popular tool – Kraken – provides an extremely rapid read classification, but its index suffers from two major limitations: an enormous memory consumption and a lossy k-mer representation through their lowest common ancestors. We present Prophyle, a metagenomic classifier based on the Burrows-Wheeler Transform. ProPhyle uses a classification algorithm similar to Kraken but with an indexing strategy based on a bottom-up propagation of k-mers in the tree, assembling contigs at each node and matching using a standard full-text search. The obtained index occupies only a fraction of RAM compared to Kraken – 13 GB instead of 90 GB for index construction and 14 GB instead of 72 GB for index querying. The resulting index is also more expressive, allowing users to retrieve a list of all genomes for every queried k-mer. Overall, ProPhyle provides an index for resource-frugal metagenomic classification, which is accurate even with single-species phylogenetic trees. Prophyle is available at http://github.com/karel-brinda/prophyle, released under the MIT license.},
	language = {eng},
	urldate = {2018-08-22},
	author = {Břinda, Karel and Salikhov, Kamil and Pignotti, Simone and Kucherov, Gregory},
	month = jul,
	year = {2017},
	doi = {10.5281/zenodo.1045427},
	file = {Zenodo Full Text PDF:/home/simone/Zotero/storage/FEJRTAEF/Břinda et al. - 2017 - ProPhyle a phylogeny-based metagenomic classifier.pdf:application/pdf}
}

@article{noauthor_identification_1981,
	title = {Identification of common molecular subsequences},
	volume = {147},
	issn = {0022-2836},
	url = {http://www.sciencedirect.com/science/article/pii/0022283681900875},
	doi = {10.1016/0022-2836(81)90087-5},
	language = {en},
	number = {1},
	urldate = {2018-08-22},
	journal = {Journal of Molecular Biology},
	month = mar,
	year = {1981},
	pages = {195--197},
	file = {Snapshot:/home/simone/Zotero/storage/V778B2E4/0022283681900875.html:text/html}
}

@article{brinda_lineage_2018,
	title = {Lineage calling can identify antibiotic resistant clones within minutes},
	copyright = {© 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/early/2018/08/29/403204},
	doi = {10.1101/403204},
	abstract = {Surveillance of circulating drug resistant bacteria is essential for healthcare providers to deliver effective empiric antibiotic therapy. However, the results of surveillance may not be available on a timescale that is optimal for guiding patient treatment. Here we present a method for inferring characteristics of an unknown bacterial sample by identifying the presence of sequence variation across the genome that is linked to a phenotype of interest, in this case drug resistance. We demonstrate an implementation of this principle using sequence k-mer content, matched to a database of known genomes. We show this technique can be applied to data from an Oxford Nanopore device in real time and is capable of identifying the presence of a known resistant strain in 5 minutes, even from a complex metagenomic sample. This flexible approach has wide application to pathogen surveillance and may be used to greatly accelerate diagnoses of resistant infections.},
	language = {en},
	urldate = {2018-08-29},
	journal = {bioRxiv},
	author = {Břinda, Karel and Callendrello, Alanna and Cowley, Lauren and Charalampous, Themoula and Lee, Robyn S. and MacFadden, Derek R. and Kucherov, Gregory and O'Grady, Justin and Baym, Michael and Hanage, William P.},
	month = aug,
	year = {2018},
	pages = {403204},
	file = {Full Text PDF:/home/simone/Zotero/storage/P3997629/Břinda et al. - 2018 - Lineage calling can identify antibiotic resistant .pdf:application/pdf;Snapshot:/home/simone/Zotero/storage/JJWSXPRA/403204.html:text/html}
}